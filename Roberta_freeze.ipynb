{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1Ks1MgI1lKgbUwJxDBmKmT6jZnxPsKjsw",
      "authorship_tag": "ABX9TyOCnI98gqfLYPNSqQ+ML4LM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MalavikaSNairr/TRACE_AI_SAMPLE/blob/main/Roberta_freeze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch transformers scikit-learn pandas tqdm\n"
      ],
      "metadata": {
        "id": "xMf-vi1RG5o4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTMYCNO-G60r",
        "outputId": "412bc029-6ebd-45e0-d82a-fe63d0d1b6f5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tXk8lfTpPhJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyxwqFNZG_5I",
        "outputId": "b7b11f70-dfef-436b-dd48-46d014c2fc07"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_clean_jsonl(path):\n",
        "    data = []\n",
        "    skipped = 0\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                obj = json.loads(line)\n",
        "                # Removed the condition `if \"text\" in obj and \"label\" in obj:`\n",
        "                # This ensures all valid JSON lines are loaded.\n",
        "                data.append(obj)\n",
        "            except json.JSONDecodeError:\n",
        "                skipped += 1\n",
        "    print(f\"Loaded {len(data)} samples, Skipped {skipped} corrupted lines\")\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "def extract_features(text):\n",
        "    words = text.split()\n",
        "    return np.array([\n",
        "        len(words),\n",
        "        np.mean([len(w) for w in words]) if words else 0,\n",
        "        text.count(\",\"),\n",
        "        text.count(\".\")\n",
        "    ], dtype=np.float32)"
      ],
      "metadata": {
        "id": "E9uEZbIjHHUn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HybridDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=256,\n",
        "                 train_mode=False, feature_stats=None):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.train_mode = train_mode\n",
        "\n",
        "        # üîë FIX: Use training set statistics for normalization\n",
        "        if feature_stats is None:\n",
        "            all_feats = np.array([extract_features(t) for t in texts])\n",
        "            self.feat_mean = all_feats.mean(axis=0)\n",
        "            self.feat_std = all_feats.std(axis=0) + 1e-6\n",
        "        else:\n",
        "            self.feat_mean, self.feat_std = feature_stats\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        enc = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        feats = extract_features(self.texts[idx])\n",
        "        feats = (feats - self.feat_mean) / self.feat_std\n",
        "\n",
        "        # üîë STRONGER augmentation during training\n",
        "        if self.train_mode:\n",
        "            feats += np.random.normal(0, 0.3, feats.shape)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": enc[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": enc[\"attention_mask\"].squeeze(),\n",
        "            \"features\": torch.tensor(feats, dtype=torch.float),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "    def get_feature_stats(self):\n",
        "        return (self.feat_mean, self.feat_std)\n"
      ],
      "metadata": {
        "id": "9xIawkgMHMrY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RobertaWithFeatures(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "\n",
        "        # üîë FREEZE early layers to prevent overfitting\n",
        "        for param in self.roberta.embeddings.parameters():\n",
        "            param.requires_grad = False\n",
        "        for layer in self.roberta.encoder.layer[:8]:\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        self.feature_fc = nn.Sequential(\n",
        "            nn.Linear(4, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "\n",
        "        # üîë SIMPLER classifier with more dropout\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(768 + 64, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(256, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(64, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, features):\n",
        "        roberta_out = self.roberta(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        ).last_hidden_state[:, 0]\n",
        "\n",
        "        feat_out = self.feature_fc(features)\n",
        "        x = torch.cat([roberta_out, feat_out], dim=1)\n",
        "        logits = self.classifier(x)\n",
        "\n",
        "        # üîë AGGRESSIVE logit clamping\n",
        "        logits = torch.clamp(logits, -2.5, 2.5)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "RfRIK9TsHQhv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    # Load dataset\n",
        "    df = load_clean_jsonl(\"final_dataset_no_emojis.jsonl\")\n",
        "\n",
        "    # üîë FIX: Validate dataset has both classes\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"VALIDATING DATASET\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    class_counts = df[\"label\"].value_counts().sort_index()\n",
        "    print(f\"Total samples: {len(df)}\")\n",
        "    print(f\"Class distribution: {class_counts.to_dict()}\")\n",
        "\n",
        "    if len(class_counts) < 2:\n",
        "        print(\"\\n‚ùå ERROR: Dataset must have BOTH classes!\")\n",
        "        print(f\"   Found only: {list(class_counts.index)}\")\n",
        "        print(f\"   Need: [0, 1]\")\n",
        "        print(\"\\nüí° Check your JSONL file:\")\n",
        "        print(\"   - Label 0 = Human-written\")\n",
        "        print(\"   - Label 1 = AI-generated\")\n",
        "        return\n",
        "\n",
        "    if 0 not in class_counts or 1 not in class_counts:\n",
        "        print(\"\\n‚ùå ERROR: Missing class label!\")\n",
        "        print(f\"   Found labels: {list(class_counts.index)}\")\n",
        "        print(f\"   Need labels: [0, 1]\")\n",
        "        return\n",
        "\n",
        "    print(\"‚úÖ Dataset validation passed!\")\n",
        "    print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "    # Split data\n",
        "    train_df, val_df = train_test_split(\n",
        "        df, test_size=0.2, stratify=df[\"label\"], random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"Training samples: {len(train_df)}\")\n",
        "    print(f\"Validation samples: {len(val_df)}\")\n",
        "    print(f\"Train distribution: {train_df['label'].value_counts().to_dict()}\")\n",
        "    print(f\"Val distribution: {val_df['label'].value_counts().to_dict()}\\n\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_ds = HybridDataset(\n",
        "        train_df.text.tolist(),\n",
        "        train_df.label.tolist(),\n",
        "        tokenizer,\n",
        "        train_mode=True\n",
        "    )\n",
        "\n",
        "    val_ds = HybridDataset(\n",
        "        val_df.text.tolist(),\n",
        "        val_df.label.tolist(),\n",
        "        tokenizer,\n",
        "        train_mode=False,\n",
        "        feature_stats=train_ds.get_feature_stats()\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=32)\n",
        "\n",
        "    # üîë FIX: Calculate class weights for BOTH classes\n",
        "    counts = train_df[\"label\"].value_counts().sort_index()\n",
        "    total = len(train_df)\n",
        "\n",
        "    # Ensure we have weights for both class 0 and class 1\n",
        "    raw_weights = []\n",
        "    for i in [0, 1]:  # Explicitly for class 0 and 1\n",
        "        if i in counts:\n",
        "            raw_weights.append(total / (2 * counts[i]))\n",
        "        else:\n",
        "            raw_weights.append(1.0)  # Default weight if class missing\n",
        "\n",
        "    # Dampen extreme weights\n",
        "    class_weights = torch.tensor(\n",
        "        [(w ** 0.5) for w in raw_weights],\n",
        "        dtype=torch.float\n",
        "    ).to(device)\n",
        "\n",
        "    print(\"Class weights (dampened):\", class_weights.tolist())\n",
        "\n",
        "    # Initialize model\n",
        "    model = RobertaWithFeatures(dropout_rate=0.6).to(device)\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-6, weight_decay=0.01)\n",
        "\n",
        "    # Loss function\n",
        "    loss_fn = nn.CrossEntropyLoss(\n",
        "        weight=class_weights,\n",
        "        label_smoothing=0.2\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"TRAINING FOR 1 EPOCH\")\n",
        "    print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=\"Training Epoch 1/1\"):\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(\n",
        "            batch[\"input_ids\"].to(device),\n",
        "            batch[\"attention_mask\"].to(device),\n",
        "            batch[\"features\"].to(device)\n",
        "        )\n",
        "        loss = loss_fn(logits, batch[\"labels\"].to(device))\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"\\nEpoch 1 Train Loss: {avg_loss:.4f}\")\n",
        "    # Validation\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"VALIDATION\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    model.eval()\n",
        "    preds, labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
        "            out = model(\n",
        "                batch[\"input_ids\"].to(device),\n",
        "                batch[\"attention_mask\"].to(device),\n",
        "                batch[\"features\"].to(device)\n",
        "            )\n",
        "            preds.extend(out.argmax(1).cpu().numpy())\n",
        "            labels.extend(batch[\"labels\"].numpy())\n",
        "\n",
        "    f1 = f1_score(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "\n",
        "    print(f\"\\n\" + \"=\" * 60)\n",
        "    print(\"FINAL RESULTS\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Validation Accuracy: {acc:.4f} ({acc*100:.2f}%)\")\n",
        "    print(f\"Validation F1 Score: {f1:.4f}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Save model\n",
        "    torch.save({\n",
        "        'model_state': model.state_dict(),\n",
        "        'feature_stats': train_ds.get_feature_stats()\n",
        "    }, \"/content/drive/MyDrive/TraceAI_Hybrid_Best.pt\")\n",
        "\n",
        "    tokenizer.save_pretrained(\"/content/drive/MyDrive/TraceAI_Hybrid_Best\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Model saved to Google Drive!\")\n",
        "    print(f\"   Location: /content/drive/MyDrive/TraceAI_Hybrid_Best.pt\")"
      ],
      "metadata": {
        "id": "6qYVdn1FMr0m"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Nn1pqBPNOr5",
        "outputId": "adca7760-cc4e-445a-fb21-80e3ef4491c5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 8264 samples, Skipped 1 corrupted lines\n",
            "\n",
            "============================================================\n",
            "VALIDATING DATASET\n",
            "============================================================\n",
            "Total samples: 8264\n",
            "Class distribution: {0: 8264}\n",
            "\n",
            "‚ùå ERROR: Dataset must have BOTH classes!\n",
            "   Found only: [0]\n",
            "   Need: [0, 1]\n",
            "\n",
            "üí° Check your JSONL file:\n",
            "   - Label 0 = Human-written\n",
            "   - Label 1 = AI-generated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "f25a9f26",
        "outputId": "a5709626-312e-4bdd-d21c-b793b8ee228d"
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "def load_jsonl_to_df(path):\n",
        "    data = []\n",
        "    skipped_count = 0\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                data.append(json.loads(line))\n",
        "            except json.JSONDecodeError:\n",
        "                skipped_count += 1\n",
        "    if skipped_count > 0:\n",
        "        print(f\"Skipped {skipped_count} corrupted lines in {path}\")\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "file_path = \"final_dataset_no_emojis.jsonl\"\n",
        "df_content = load_jsonl_to_df(file_path)\n",
        "\n",
        "print(f\"Content of {file_path}:\")\n",
        "display(df_content.head())\n",
        "print(f\"Total entries: {len(df_content)}\")\n",
        "print(f\"Value counts for 'label':\\n{df_content['label'].value_counts()}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content of final_dataset_no_emojis.jsonl:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                text  label\n",
              "0  the yangtze giant softshell turtle rafetus swi...      0\n",
              "1  propensity scores are typically used in the ma...      0\n",
              "2  cv using full set for model selection huh it s...      0\n",
              "3  gift my closest people enough money to love co...      0\n",
              "4  one example of an observational study was run ...      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-882b77e9-8bd4-46f2-929d-b21d3057d3f0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>the yangtze giant softshell turtle rafetus swi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>propensity scores are typically used in the ma...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cv using full set for model selection huh it s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>gift my closest people enough money to love co...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>one example of an observational study was run ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-882b77e9-8bd4-46f2-929d-b21d3057d3f0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-882b77e9-8bd4-46f2-929d-b21d3057d3f0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-882b77e9-8bd4-46f2-929d-b21d3057d3f0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-64dfd6db-1f76-4212-9711-293f216d3d76\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-64dfd6db-1f76-4212-9711-293f216d3d76')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-64dfd6db-1f76-4212-9711-293f216d3d76 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(f\\\"Value counts for 'label':\\\\n{df_content['label']\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"propensity scores are typically used in the matching literature propensity scores use pre treatment covariates to estimate the probability of receiving treatment essentially a regression either just regular ols or logit probit etc is used to calculate the propensity score with treatment as your outcome and pre treatment variables are your covariates once a good estimate of the propensity score is obtained subjects with similar propensity scores but different treatments received are matched to one another the treatment effect is the difference in means between these two groups rosenbaum and rubin 1983 show that matching treated and control subjects using just the propensity score is sufficient to remove all bias in the estimate of the treatment effect stemming from the observed pre treatment covariates used to construct the score note that this proof requires the use of the true propensity score rather than an estimate the advantage of this approach is it turns a problem of matching in multiple dimensions one for each pre treatment covariate into a univariate matching case a great simplification rosenbaum paul r and donald b rubin 1983 the central role of the propensity score in observational studies for causal effects biometrika 70 1 41 55\",\n          \"one example of an observational study was run by arthur bandura this observational study focused on children who were exposed to an adult exhibiting aggressive behaviors and their reaction to toys versus other children who were not exposed to these stimuli the result shows that children who had seen the adult acting aggressively towards a toy in turn were aggressive towards their own toy when put in a situation that frustrated them\",\n          \"cv using full set for model selection huh it s a common error still even wikipedia mentions it because it is a hidden overfit you need to make a higher level cv or leave some test to do this right i d love to see this loss minimizing in clustering knn or random ferns have you voted for close it is strange that since without shane we couldn t close any question that would be true if everyone were knowledgeable enough in statistics to drive the correct conclusions alas that quote is very applicable to many of those amusing human beings called politicians i do think this could be rephrased in such a way that it would be on topic eg about the kind of work that statistical consultants do but this is more like a job request great post note that vapnick had a phd in statistics i m not sure there are a lot of computer scientist that know the name talagrand and i m sure 0 01 of them can state by memory one result of talagrand can you i don t know the work of valiant\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total entries: 18512\n",
            "Value counts for 'label':\n",
            "label\n",
            "0    12788\n",
            "1     5724\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FINAL_ROBERTA_CODE**"
      ],
      "metadata": {
        "id": "iqYOHfdgPjbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch transformers scikit-learn pandas tqdm"
      ],
      "metadata": {
        "id": "ihXTN71nPnKG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dMr-7_OPzxI",
        "outputId": "e5528ec8-9bcd-4a1b-8e63-52d8d81dc175"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCxZ29DGP31U",
        "outputId": "d40c5298-f93f-4410-b61c-562f8cff92e8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_clean_jsonl(path):\n",
        "    \"\"\"Load JSONL file - FAILS if ANY line is corrupted\"\"\"\n",
        "    data = []\n",
        "\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line_num, line in enumerate(f, 1):\n",
        "            line = line.strip()\n",
        "            if not line:  # Skip empty lines\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                obj = json.loads(line)\n",
        "\n",
        "                # Validate required fields\n",
        "                if \"text\" not in obj:\n",
        "                    raise ValueError(f\"Line {line_num}: Missing 'text' field\")\n",
        "                if \"label\" not in obj:\n",
        "                    raise ValueError(f\"Line {line_num}: Missing 'label' field\")\n",
        "\n",
        "                # Convert label to integer\n",
        "                obj[\"label\"] = int(obj[\"label\"])\n",
        "\n",
        "                # Validate label values\n",
        "                if obj[\"label\"] not in [0, 1]:\n",
        "                    raise ValueError(f\"Line {line_num}: Label must be 0 or 1, got {obj['label']}\")\n",
        "\n",
        "                data.append(obj)\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                raise ValueError(f\"Line {line_num}: Invalid JSON - {str(e)}\")\n",
        "            except ValueError as e:\n",
        "                raise ValueError(f\"Line {line_num}: {str(e)}\")\n",
        "\n",
        "    if len(data) == 0:\n",
        "        raise ValueError(\"No valid data found in file!\")\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    print(f\"‚úÖ Successfully loaded {len(df)} samples\")\n",
        "    print(f\"Class distribution: {df['label'].value_counts().to_dict()}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def extract_features(text):\n",
        "    \"\"\"Extract statistical features from text\"\"\"\n",
        "    words = text.split()\n",
        "    return np.array([\n",
        "        len(words),\n",
        "        np.mean([len(w) for w in words]) if words else 0,\n",
        "        text.count(\",\"),\n",
        "        text.count(\".\")\n",
        "    ], dtype=np.float32)"
      ],
      "metadata": {
        "id": "hlbTfF5_QAPE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HybridDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=256,\n",
        "                 train_mode=False, feature_stats=None):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.train_mode = train_mode\n",
        "\n",
        "        # Use training set statistics for normalization\n",
        "        if feature_stats is None:\n",
        "            all_feats = np.array([extract_features(t) for t in texts])\n",
        "            self.feat_mean = all_feats.mean(axis=0)\n",
        "            self.feat_std = all_feats.std(axis=0) + 1e-6\n",
        "        else:\n",
        "            self.feat_mean, self.feat_std = feature_stats\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        enc = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        feats = extract_features(self.texts[idx])\n",
        "        feats = (feats - self.feat_mean) / self.feat_std\n",
        "\n",
        "        # Stronger augmentation during training\n",
        "        if self.train_mode:\n",
        "            feats += np.random.normal(0, 0.3, feats.shape)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": enc[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": enc[\"attention_mask\"].squeeze(),\n",
        "            \"features\": torch.tensor(feats, dtype=torch.float),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "    def get_feature_stats(self):\n",
        "        return (self.feat_mean, self.feat_std)\n"
      ],
      "metadata": {
        "id": "Co2348q0QICh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RobertaWithFeatures(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "\n",
        "        # Freeze early layers to prevent overfitting\n",
        "        for param in self.roberta.embeddings.parameters():\n",
        "            param.requires_grad = False\n",
        "        for layer in self.roberta.encoder.layer[:8]:\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        self.feature_fc = nn.Sequential(\n",
        "            nn.Linear(4, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "\n",
        "        # Classifier with strong dropout\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(768 + 64, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(256, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(64, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, features):\n",
        "        roberta_out = self.roberta(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        ).last_hidden_state[:, 0]\n",
        "\n",
        "        feat_out = self.feature_fc(features)\n",
        "        x = torch.cat([roberta_out, feat_out], dim=1)\n",
        "        logits = self.classifier(x)\n",
        "\n",
        "        # Aggressive logit clamping\n",
        "        logits = torch.clamp(logits, -2.5, 2.5)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "lvlBqyVOQL20"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    print(\"=\" * 60)\n",
        "    print(\"LOADING DATASET\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Load dataset - will fail if any line is corrupted\n",
        "    try:\n",
        "        df = load_clean_jsonl(\"final_dataset_no_emojis.jsonl\")\n",
        "    except ValueError as e:\n",
        "        print(f\"\\n‚ùå ERROR: {str(e)}\")\n",
        "        print(\"\\nüí° Fix your JSONL file and try again.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"VALIDATING DATASET\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    class_counts = df[\"label\"].value_counts().sort_index()\n",
        "    print(f\"Total samples: {len(df)}\")\n",
        "    print(f\"Class 0 (Human): {class_counts.get(0, 0)}\")\n",
        "    print(f\"Class 1 (AI): {class_counts.get(1, 0)}\")\n",
        "\n",
        "    # Validate both classes exist\n",
        "    if len(class_counts) < 2:\n",
        "        print(\"\\n‚ùå ERROR: Dataset must have BOTH classes!\")\n",
        "        print(f\"   Found only: {list(class_counts.index)}\")\n",
        "        print(f\"   Need: [0, 1]\")\n",
        "        return\n",
        "\n",
        "    if 0 not in class_counts or 1 not in class_counts:\n",
        "        print(\"\\n‚ùå ERROR: Missing class label!\")\n",
        "        print(f\"   Found labels: {list(class_counts.index)}\")\n",
        "        print(f\"   Need labels: [0, 1]\")\n",
        "        return\n",
        "\n",
        "    print(\"‚úÖ Dataset validation passed!\")\n",
        "\n",
        "    # Split data\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"SPLITTING DATA\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    train_df, val_df = train_test_split(\n",
        "        df, test_size=0.2, stratify=df[\"label\"], random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"Training samples: {len(train_df)}\")\n",
        "    print(f\"  - Class 0: {(train_df['label'] == 0).sum()}\")\n",
        "    print(f\"  - Class 1: {(train_df['label'] == 1).sum()}\")\n",
        "    print(f\"Validation samples: {len(val_df)}\")\n",
        "    print(f\"  - Class 0: {(val_df['label'] == 0).sum()}\")\n",
        "    print(f\"  - Class 1: {(val_df['label'] == 1).sum()}\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"LOADING TOKENIZER\")\n",
        "    print(\"=\" * 60)\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "    print(\"‚úÖ Tokenizer loaded\")\n",
        "\n",
        "    # Create datasets\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"CREATING DATASETS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    train_ds = HybridDataset(\n",
        "        train_df.text.tolist(),\n",
        "        train_df.label.tolist(),\n",
        "        tokenizer,\n",
        "        train_mode=True\n",
        "    )\n",
        "\n",
        "    val_ds = HybridDataset(\n",
        "        val_df.text.tolist(),\n",
        "        val_df.label.tolist(),\n",
        "        tokenizer,\n",
        "        train_mode=False,\n",
        "        feature_stats=train_ds.get_feature_stats()\n",
        "    )\n",
        "\n",
        "    print(\"‚úÖ Datasets created\")\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=32)\n",
        "\n",
        "    # Calculate class weights\n",
        "    counts = train_df[\"label\"].value_counts().sort_index()\n",
        "    total = len(train_df)\n",
        "\n",
        "    raw_weights = []\n",
        "    for i in [0, 1]:\n",
        "        if i in counts:\n",
        "            raw_weights.append(total / (2 * counts[i]))\n",
        "        else:\n",
        "            raw_weights.append(1.0)\n",
        "\n",
        "    # Dampen extreme weights\n",
        "    class_weights = torch.tensor(\n",
        "        [(w ** 0.5) for w in raw_weights],\n",
        "        dtype=torch.float\n",
        "    ).to(device)\n",
        "\n",
        "    print(f\"\\nClass weights (dampened): {class_weights.tolist()}\")\n",
        "\n",
        "    # Initialize model\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"INITIALIZING MODEL\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    model = RobertaWithFeatures(dropout_rate=0.6).to(device)\n",
        "    print(\"‚úÖ Model initialized\")\n",
        "\n",
        "    # Count trainable parameters\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"Frozen parameters: {total_params - trainable_params:,}\")\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-6, weight_decay=0.01)\n",
        "\n",
        "    # Loss function\n",
        "    loss_fn = nn.CrossEntropyLoss(\n",
        "        weight=class_weights,\n",
        "        label_smoothing=0.2\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"TRAINING (1 EPOCH)\")\n",
        "    print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    num_batches = len(train_loader)\n",
        "\n",
        "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(\n",
        "            batch[\"input_ids\"].to(device),\n",
        "            batch[\"attention_mask\"].to(device),\n",
        "            batch[\"features\"].to(device)\n",
        "        )\n",
        "\n",
        "        loss = loss_fn(logits, batch[\"labels\"].to(device))\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    print(f\"\\nTrain Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"VALIDATION\")\n",
        "    print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "    model.eval()\n",
        "    preds, labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
        "            out = model(\n",
        "                batch[\"input_ids\"].to(device),\n",
        "                batch[\"attention_mask\"].to(device),\n",
        "                batch[\"features\"].to(device)\n",
        "            )\n",
        "            preds.extend(out.argmax(1).cpu().numpy())\n",
        "            labels.extend(batch[\"labels\"].numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    f1 = f1_score(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "\n",
        "    # Per-class accuracy\n",
        "    labels_np = np.array(labels)\n",
        "    preds_np = np.array(preds)\n",
        "\n",
        "    class0_mask = labels_np == 0\n",
        "    class1_mask = labels_np == 1\n",
        "\n",
        "    class0_acc = accuracy_score(labels_np[class0_mask], preds_np[class0_mask])\n",
        "    class1_acc = accuracy_score(labels_np[class1_mask], preds_np[class1_mask])\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"FINAL RESULTS\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Overall Accuracy: {acc:.4f} ({acc*100:.2f}%)\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Class 0 (Human) Accuracy: {class0_acc:.4f} ({class0_acc*100:.2f}%)\")\n",
        "    print(f\"Class 1 (AI) Accuracy: {class1_acc:.4f} ({class1_acc*100:.2f}%)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Save model\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"SAVING MODEL\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    torch.save({\n",
        "        'model_state': model.state_dict(),\n",
        "        'feature_stats': train_ds.get_feature_stats(),\n",
        "        'accuracy': acc,\n",
        "        'f1_score': f1\n",
        "    }, \"/content/drive/MyDrive/TraceAI_Hybrid_Best.pt\")\n",
        "\n",
        "    tokenizer.save_pretrained(\"/content/drive/MyDrive/TraceAI_Hybrid_Best\")\n",
        "\n",
        "    print(f\"‚úÖ Model saved to: /content/drive/MyDrive/TraceAI_Hybrid_Best.pt\")\n",
        "    print(f\"‚úÖ Tokenizer saved to: /content/drive/MyDrive/TraceAI_Hybrid_Best/\")"
      ],
      "metadata": {
        "id": "QNwImnGFQS8Q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJgqVzQqQVpp",
        "outputId": "b14aa2d8-ffb6-4173-9dfb-6e4a9491597c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "LOADING DATASET\n",
            "============================================================\n",
            "‚úÖ Successfully loaded 20788 samples\n",
            "Class distribution: {0: 12788, 1: 8000}\n",
            "\n",
            "============================================================\n",
            "VALIDATING DATASET\n",
            "============================================================\n",
            "Total samples: 20788\n",
            "Class 0 (Human): 12788\n",
            "Class 1 (AI): 8000\n",
            "‚úÖ Dataset validation passed!\n",
            "\n",
            "============================================================\n",
            "SPLITTING DATA\n",
            "============================================================\n",
            "Training samples: 16630\n",
            "  - Class 0: 10230\n",
            "  - Class 1: 6400\n",
            "Validation samples: 4158\n",
            "  - Class 0: 2558\n",
            "  - Class 1: 1600\n",
            "\n",
            "============================================================\n",
            "LOADING TOKENIZER\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Tokenizer loaded\n",
            "\n",
            "============================================================\n",
            "CREATING DATASETS\n",
            "============================================================\n",
            "‚úÖ Datasets created\n",
            "\n",
            "Class weights (dampened): [0.9015572667121887, 1.1398327350616455]\n",
            "\n",
            "============================================================\n",
            "INITIALIZING MODEL\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model initialized\n",
            "Trainable parameters: 29,172,226\n",
            "Frozen parameters: 95,703,552\n",
            "\n",
            "============================================================\n",
            "TRAINING (1 EPOCH)\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1040/1040 [06:44<00:00,  2.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.5059\n",
            "\n",
            "============================================================\n",
            "VALIDATION\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 130/130 [00:58<00:00,  2.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "FINAL RESULTS\n",
            "============================================================\n",
            "Overall Accuracy: 0.9947 (99.47%)\n",
            "F1 Score: 0.9931\n",
            "Class 0 (Human) Accuracy: 0.9953 (99.53%)\n",
            "Class 1 (AI) Accuracy: 0.9938 (99.38%)\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "SAVING MODEL\n",
            "============================================================\n",
            "‚úÖ Model saved to: /content/drive/MyDrive/TraceAI_Hybrid_Best.pt\n",
            "‚úÖ Tokenizer saved to: /content/drive/MyDrive/TraceAI_Hybrid_Best/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def infer(text, temperature=3.0):\n",
        "    \"\"\"\n",
        "    Classify text as Human, AI, or Uncertain.\n",
        "\n",
        "    Args:\n",
        "        text: Input text to classify\n",
        "        temperature: Higher = more uncertain (try 2.5-4.0)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with prediction, probabilities, and confidence\n",
        "    \"\"\"\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(\n",
        "        \"/content/drive/MyDrive/TraceAI_Hybrid_Best\"\n",
        "    )\n",
        "\n",
        "    # FIX: Add weights_only=False to allow loading of feature_stats (numpy arrays)\n",
        "    checkpoint = torch.load(\"/content/drive/MyDrive/TraceAI_Hybrid_Best.pt\", weights_only=False)\n",
        "\n",
        "    model = RobertaWithFeatures().to(device)\n",
        "    model.load_state_dict(checkpoint['model_state'])\n",
        "    model.eval()\n",
        "\n",
        "    enc = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=256\n",
        "    ).to(device)\n",
        "\n",
        "    # Use saved training statistics\n",
        "    feat_mean, feat_std = checkpoint['feature_stats']\n",
        "    feats = extract_features(text)\n",
        "    feats = (feats - feat_mean) / feat_std\n",
        "    feats = torch.tensor(feats, dtype=torch.float).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(enc[\"input_ids\"], enc[\"attention_mask\"], feats)\n",
        "\n",
        "    # Strong temperature scaling\n",
        "    logits = logits / temperature\n",
        "    probs = torch.softmax(logits, dim=1)[0]\n",
        "\n",
        "    human_p, ai_p = probs[0].item(), probs[1].item()\n",
        "\n",
        "    # Wider uncertainty threshold\n",
        "    if abs(ai_p - human_p) < 0.30:\n",
        "        label = \"UNCERTAIN\"\n",
        "    elif ai_p > human_p:\n",
        "        label = \"AI\"\n",
        "    else:\n",
        "        label = \"HUMAN\"\n",
        "\n",
        "    return {\n",
        "        \"Prediction\": label,\n",
        "        \"AI %\": round(ai_p * 100, 2),\n",
        "        \"Human %\": round(human_p * 100, 2),\n",
        "        \"Confidence\": round(abs(ai_p - human_p) * 100, 2)\n",
        "    }"
      ],
      "metadata": {
        "id": "uTAVKPdhQbdL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = input(\"Enter text to analyze: \")\n",
        "result = infer(text, temperature=3.0)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ANALYSIS RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Prediction: {result['Prediction']}\")\n",
        "print(f\"AI Probability: {result['AI %']}%\")\n",
        "print(f\"Human Probability: {result['Human %']}%\")\n",
        "print(f\"Confidence Gap: {result['Confidence']}%\")\n",
        "print(\"=\" * 60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r59Y7mqVRTI5",
        "outputId": "8f8e8eeb-1379-4108-a3c0-8f658564fdb5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter text to analyze: What makes me happy: My Mum, Dad, Grandparents (both maternal and paternal),  The Sun, trees, flowers, the sky, the beach, my house, my bed, sausages, chicken, pretzels, Love, School, my ex, Bunnies, Cats, Dogs, Chickens, Peppa Pig, listening to Lil Nas X, Driving, Telling Jokes, Scooting, Shopping, Concerts, Day outs, My sister, My cousins, my aunts and uncles, Meatballs, Potatoes, History, Archery, Geography, Math, Reading, Singing, Dancing, Farting, Comedy, MrBeast, Hikes, Making Memes, Saxophones, Shrek, The Minions, Playing the Piano, X(when it was Twitter), Facebook, Instagram, Michael Jackson, Typing, Sofas, Museums, Castles, My singing monsters, Elton John‚Äôs Music, IShowSpeed, The Mario Movie, Roses, Daisies, Mickey Mouse etc.  The World is such a happy place. Isn‚Äôt that right? \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ANALYSIS RESULTS\n",
            "============================================================\n",
            "Prediction: AI\n",
            "AI Probability: 70.9%\n",
            "Human Probability: 29.1%\n",
            "Confidence Gap: 41.8%\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with the samples you provided\n",
        "sample_texts = [\n",
        "    # Human text (label 0)\n",
        "    \"i think it depends on which hypothesis testing you are talking about the classical hypothesis testing neyman pearson is said to be defective because it does not appropriately condition on what actually happened\",\n",
        "\n",
        "    # Human text (label 0)\n",
        "    \"i have no desire to work at a fast food counter and have avoided it most of my life the put offs would be the itchy hot polyester uniforms grease and loud disgruntled customers\",\n",
        "\n",
        "    # AI text (label 1)\n",
        "    \"Limiting car usage has numerous advantages for both individuals and society as a whole. The passages provide examples of communities and cities that have implemented measures to promote alternative transportation and reduce car dependence.\"\n",
        "]\n",
        "\n",
        "print(\"TESTING WITH SAMPLE DATA\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, text in enumerate(sample_texts, 1):\n",
        "    result = infer(text, temperature=3.0)\n",
        "    print(f\"\\nüìù Sample {i}:\")\n",
        "    print(f\"Text: {text[:100]}{'...' if len(text) > 100 else ''}\")\n",
        "    print(f\"‚Üí {result['Prediction']} | AI: {result['AI %']}% | Human: {result['Human %']}% | Confidence: {result['Confidence']}%\")\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtbmTB5cRY_X",
        "outputId": "b2a7e775-65f3-4c62-f5dc-a3baf4b6205c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TESTING WITH SAMPLE DATA\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìù Sample 1:\n",
            "Text: i think it depends on which hypothesis testing you are talking about the classical hypothesis testin...\n",
            "‚Üí HUMAN | AI: 33.22% | Human: 66.78% | Confidence: 33.56%\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìù Sample 2:\n",
            "Text: i have no desire to work at a fast food counter and have avoided it most of my life the put offs wou...\n",
            "‚Üí HUMAN | AI: 33.25% | Human: 66.75% | Confidence: 33.49%\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìù Sample 3:\n",
            "Text: Limiting car usage has numerous advantages for both individuals and society as a whole. The passages...\n",
            "‚Üí AI | AI: 70.18% | Human: 29.82% | Confidence: 40.36%\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test multiple texts\n",
        "test_texts = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"In accordance with the aforementioned stipulations, it is hereby declared that all parties must comply with the regulatory framework.\",\n",
        "    \"I went to the store yesterday and bought some milk, bread, and eggs for dinner.\",\n",
        "    \"The implementation of artificial intelligence systems requires careful consideration of ethical implications and potential societal impacts.\",\n",
        "    \"hey whats up? nothing much just chilling at home lol\"\n",
        "]\n",
        "\n",
        "print(\"BATCH TESTING RESULTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, text in enumerate(test_texts, 1):\n",
        "    result = infer(text, temperature=3.0)\n",
        "    print(f\"\\nüìù Test {i}:\")\n",
        "    print(f\"Text: {text[:70]}{'...' if len(text) > 70 else ''}\")\n",
        "    print(f\"‚Üí {result['Prediction']} | AI: {result['AI %']}% | Human: {result['Human %']}% | Confidence: {result['Confidence']}%\")\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6KwBH3NRb_p",
        "outputId": "d47e793e-0073-49f4-f0bb-eed1b0c011dd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BATCH TESTING RESULTS\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìù Test 1:\n",
            "Text: The quick brown fox jumps over the lazy dog.\n",
            "‚Üí AI | AI: 68.2% | Human: 31.8% | Confidence: 36.4%\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìù Test 2:\n",
            "Text: In accordance with the aforementioned stipulations, it is hereby decla...\n",
            "‚Üí AI | AI: 69.79% | Human: 30.21% | Confidence: 39.57%\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìù Test 3:\n",
            "Text: I went to the store yesterday and bought some milk, bread, and eggs fo...\n",
            "‚Üí AI | AI: 68.31% | Human: 31.69% | Confidence: 36.62%\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìù Test 4:\n",
            "Text: The implementation of artificial intelligence systems requires careful...\n",
            "‚Üí AI | AI: 70.22% | Human: 29.78% | Confidence: 40.43%\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìù Test 5:\n",
            "Text: hey whats up? nothing much just chilling at home lol\n",
            "‚Üí UNCERTAIN | AI: 40.05% | Human: 59.95% | Confidence: 19.9%\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 13: Display Saved Model Metrics\n",
        "def display_saved_metrics():\n",
        "    \"\"\"\n",
        "    Load and display all metrics saved during training\n",
        "    \"\"\"\n",
        "    import torch\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"LOADING SAVED MODEL METRICS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    try:\n",
        "        checkpoint = torch.load(\"/content/drive/MyDrive/TraceAI_Hybrid_Best.pt\")\n",
        "\n",
        "        # Extract metrics\n",
        "        accuracy = checkpoint.get('accuracy', 'N/A')\n",
        "        f1_score = checkpoint.get('f1_score', 'N/A')\n",
        "\n",
        "        confusion_matrix = checkpoint.get('confusion_matrix', {})\n",
        "        tn = confusion_matrix.get('true_negatives', 0)\n",
        "        fp = confusion_matrix.get('false_positives', 0)\n",
        "        fn = confusion_matrix.get('false_negatives', 0)\n",
        "        tp = confusion_matrix.get('true_positives', 0)\n",
        "\n",
        "        rates = checkpoint.get('rates', {})\n",
        "        tpr = rates.get('true_positive_rate', 0)\n",
        "        fpr = rates.get('false_positive_rate', 0)\n",
        "        tnr = rates.get('true_negative_rate', 0)\n",
        "        fnr = rates.get('false_negative_rate', 0)\n",
        "\n",
        "        metrics = checkpoint.get('metrics', {})\n",
        "        precision = metrics.get('precision', 0)\n",
        "        recall = metrics.get('recall', 0)\n",
        "\n",
        "        # Display results\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"CONFUSION MATRIX\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"{'':20s} Predicted\")\n",
        "        print(f\"{'':20s} {'Human':>10s} {'AI':>10s}\")\n",
        "        print(f\"{'Actual Human':20s} {tn:10d} {fp:10d}\")\n",
        "        print(f\"{'Actual AI':20s} {fn:10d} {tp:10d}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"COUNT METRICS\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"{'True Negatives (TN)':40s} {tn:8d}  (Human correctly identified)\")\n",
        "        print(f\"{'False Positives (FP)':40s} {fp:8d}  (Human wrongly labeled as AI)\")\n",
        "        print(f\"{'False Negatives (FN)':40s} {fn:8d}  (AI wrongly labeled as Human)\")\n",
        "        print(f\"{'True Positives (TP)':40s} {tp:8d}  (AI correctly identified)\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"RATE METRICS (Percentages)\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"{'True Positive Rate (Sensitivity/Recall)':40s} {tpr:7.4f}  ({tpr*100:6.2f}%)\")\n",
        "        print(f\"{'False Positive Rate':40s} {fpr:7.4f}  ({fpr*100:6.2f}%)\")\n",
        "        print(f\"{'True Negative Rate (Specificity)':40s} {tnr:7.4f}  ({tnr*100:6.2f}%)\")\n",
        "        print(f\"{'False Negative Rate':40s} {fnr:7.4f}  ({fnr*100:6.2f}%)\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"OVERALL PERFORMANCE METRICS\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"{'Overall Accuracy':40s} {accuracy:7.4f}  ({accuracy*100:6.2f}%)\")\n",
        "        print(f\"{'F1 Score':40s} {f1_score:7.4f}\")\n",
        "        print(f\"{'Precision':40s} {precision:7.4f}  ({precision*100:6.2f}%)\")\n",
        "        print(f\"{'Recall':40s} {recall:7.4f}  ({recall*100:6.2f}%)\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"INTERPRETATION GUIDE\")\n",
        "        print(\"=\" * 80)\n",
        "        print(\"\"\"\n",
        "üìä Key Metrics Explained:\n",
        "\n",
        "TRUE POSITIVE RATE (Sensitivity/Recall):\n",
        "   ‚Üí How well the model identifies AI-generated text\n",
        "   ‚Üí Higher is better (ideally > 0.85)\n",
        "\n",
        "FALSE POSITIVE RATE:\n",
        "   ‚Üí How often human text is misclassified as AI\n",
        "   ‚Üí Lower is better (ideally < 0.15)\n",
        "\n",
        "TRUE NEGATIVE RATE (Specificity):\n",
        "   ‚Üí How well the model identifies human-written text\n",
        "   ‚Üí Higher is better (ideally > 0.85)\n",
        "\n",
        "FALSE NEGATIVE RATE:\n",
        "   ‚Üí How often AI text is misclassified as human\n",
        "   ‚Üí Lower is better (ideally < 0.15)\n",
        "\n",
        "PRECISION:\n",
        "   ‚Üí When model predicts AI, how often is it correct?\n",
        "   ‚Üí Higher is better\n",
        "\n",
        "RECALL (same as TPR):\n",
        "   ‚Üí Of all actual AI texts, how many did we catch?\n",
        "   ‚Üí Higher is better\n",
        "\n",
        "F1 SCORE:\n",
        "   ‚Üí Balanced measure of precision and recall\n",
        "   ‚Üí Higher is better (ideally > 0.80)\n",
        "        \"\"\")\n",
        "\n",
        "        print(\"=\" * 80)\n",
        "        print(\"‚úÖ METRICS LOADED SUCCESSFULLY\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"\\n‚ùå ERROR: Model file not found!\")\n",
        "        print(\"   Please train the model first using the train() function.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå ERROR: {str(e)}\")\n",
        "\n",
        "# Run the function\n",
        "display_saved_metrics()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpvOjTrwRfSL",
        "outputId": "0fb27c89-6720-4a9a-aea7-7d0e734c700f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "LOADING SAVED MODEL METRICS\n",
            "================================================================================\n",
            "\n",
            "‚ùå ERROR: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
            "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
            "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
            "\tWeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy._core.multiarray._reconstruct])` or the `torch.serialization.safe_globals([numpy._core.multiarray._reconstruct])` context manager to allowlist this global if you trust this class/function.\n",
            "\n",
            "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 13: Display Saved Model Metrics\n",
        "# ============================================================================\n",
        "# Run this cell to see all training results and metrics\n",
        "# ============================================================================\n",
        "\n",
        "import torch\n",
        "\n",
        "def display_saved_metrics():\n",
        "    \"\"\"\n",
        "    Load and display all metrics saved during training\n",
        "    \"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"LOADING SAVED MODEL METRICS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    try:\n",
        "        # Load checkpoint with weights_only=False to handle numpy arrays\n",
        "        checkpoint = torch.load(\n",
        "            \"/content/drive/MyDrive/TraceAI_Hybrid_Best.pt\",\n",
        "            map_location=device,\n",
        "            weights_only=False\n",
        "        )\n",
        "\n",
        "        # Extract metrics\n",
        "        accuracy = checkpoint.get('accuracy', 'N/A')\n",
        "        f1_score = checkpoint.get('f1_score', 'N/A')\n",
        "\n",
        "        confusion_matrix = checkpoint.get('confusion_matrix', {})\n",
        "        tn = confusion_matrix.get('true_negatives', 0)\n",
        "        fp = confusion_matrix.get('false_positives', 0)\n",
        "        fn = confusion_matrix.get('false_negatives', 0)\n",
        "        tp = confusion_matrix.get('true_positives', 0)\n",
        "\n",
        "        rates = checkpoint.get('rates', {})\n",
        "        tpr = rates.get('true_positive_rate', 0)\n",
        "        fpr = rates.get('false_positive_rate', 0)\n",
        "        tnr = rates.get('true_negative_rate', 0)\n",
        "        fnr = rates.get('false_negative_rate', 0)\n",
        "\n",
        "        metrics = checkpoint.get('metrics', {})\n",
        "        precision = metrics.get('precision', 0)\n",
        "        recall = metrics.get('recall', 0)\n",
        "\n",
        "        # Calculate total samples\n",
        "        total_samples = tn + fp + fn + tp\n",
        "\n",
        "        # Display results\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"CONFUSION MATRIX\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"{'':20s} Predicted\")\n",
        "        print(f\"{'':20s} {'Human':>10s} {'AI':>10s}\")\n",
        "        print(f\"{'Actual Human':20s} {tn:10d} {fp:10d}\")\n",
        "        print(f\"{'Actual AI':20s} {fn:10d} {tp:10d}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"COUNT METRICS\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"{'True Negatives (TN)':40s} {tn:8d}  (Human correctly identified)\")\n",
        "        print(f\"{'False Positives (FP)':40s} {fp:8d}  (Human wrongly labeled as AI)\")\n",
        "        print(f\"{'False Negatives (FN)':40s} {fn:8d}  (AI wrongly labeled as Human)\")\n",
        "        print(f\"{'True Positives (TP)':40s} {tp:8d}  (AI correctly identified)\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"RATE METRICS (Percentages)\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"{'True Positive Rate (Sensitivity/Recall)':40s} {tpr:7.4f}  ({tpr*100:6.2f}%)\")\n",
        "        print(f\"{'False Positive Rate':40s} {fpr:7.4f}  ({fpr*100:6.2f}%)\")\n",
        "        print(f\"{'True Negative Rate (Specificity)':40s} {tnr:7.4f}  ({tnr*100:6.2f}%)\")\n",
        "        print(f\"{'False Negative Rate':40s} {fnr:7.4f}  ({fnr*100:6.2f}%)\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"OVERALL PERFORMANCE METRICS\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"{'Overall Accuracy':40s} {accuracy:7.4f}  ({accuracy*100:6.2f}%)\")\n",
        "        print(f\"{'F1 Score':40s} {f1_score:7.4f}\")\n",
        "        print(f\"{'Precision':40s} {precision:7.4f}  ({precision*100:6.2f}%)\")\n",
        "        print(f\"{'Recall':40s} {recall:7.4f}  ({recall*100:6.2f}%)\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"SUMMARY STATISTICS\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"{'Total Validation Samples':40s} {total_samples:8d}\")\n",
        "        print(f\"{'Correctly Classified':40s} {tn + tp:8d}  ({((tn+tp)/total_samples)*100:6.2f}%)\")\n",
        "        print(f\"{'Misclassified':40s} {fp + fn:8d}  ({((fp+fn)/total_samples)*100:6.2f}%)\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"INTERPRETATION GUIDE\")\n",
        "        print(\"=\" * 80)\n",
        "        print(\"\"\"\n",
        "üìä Key Metrics Explained:\n",
        "\n",
        "TRUE POSITIVE RATE (Sensitivity/Recall):\n",
        "   ‚Üí How well the model identifies AI-generated text\n",
        "   ‚Üí Higher is better (ideally > 0.85)\n",
        "\n",
        "FALSE POSITIVE RATE:\n",
        "   ‚Üí How often human text is misclassified as AI\n",
        "   ‚Üí Lower is better (ideally < 0.15)\n",
        "\n",
        "TRUE NEGATIVE RATE (Specificity):\n",
        "   ‚Üí How well the model identifies human-written text\n",
        "   ‚Üí Higher is better (ideally > 0.85)\n",
        "\n",
        "FALSE NEGATIVE RATE:\n",
        "   ‚Üí How often AI text is misclassified as human\n",
        "   ‚Üí Lower is better (ideally < 0.15)\n",
        "\n",
        "PRECISION:\n",
        "   ‚Üí When model predicts AI, how often is it correct?\n",
        "   ‚Üí Higher is better\n",
        "\n",
        "RECALL (same as TPR):\n",
        "   ‚Üí Of all actual AI texts, how many did we catch?\n",
        "   ‚Üí Higher is better\n",
        "\n",
        "F1 SCORE:\n",
        "   ‚Üí Balanced measure of precision and recall\n",
        "   ‚Üí Higher is better (ideally > 0.80)\n",
        "        \"\"\")\n",
        "\n",
        "        print(\"=\" * 80)\n",
        "        print(\"‚úÖ METRICS LOADED SUCCESSFULLY\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"\\n‚ùå ERROR: Model file not found!\")\n",
        "        print(\"   Path: /content/drive/MyDrive/TraceAI_Hybrid_Best.pt\")\n",
        "        print(\"   Please train the model first using the train() function.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå ERROR: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# Run the function\n",
        "display_saved_metrics()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6RifJo2W8qO",
        "outputId": "fbb62905-9004-40e2-8cbf-ea402bb5a95b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "LOADING SAVED MODEL METRICS\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "CONFUSION MATRIX\n",
            "================================================================================\n",
            "                     Predicted\n",
            "                          Human         AI\n",
            "Actual Human                  0          0\n",
            "Actual AI                     0          0\n",
            "\n",
            "================================================================================\n",
            "COUNT METRICS\n",
            "================================================================================\n",
            "True Negatives (TN)                             0  (Human correctly identified)\n",
            "False Positives (FP)                            0  (Human wrongly labeled as AI)\n",
            "False Negatives (FN)                            0  (AI wrongly labeled as Human)\n",
            "True Positives (TP)                             0  (AI correctly identified)\n",
            "\n",
            "================================================================================\n",
            "RATE METRICS (Percentages)\n",
            "================================================================================\n",
            "True Positive Rate (Sensitivity/Recall)   0.0000  (  0.00%)\n",
            "False Positive Rate                       0.0000  (  0.00%)\n",
            "True Negative Rate (Specificity)          0.0000  (  0.00%)\n",
            "False Negative Rate                       0.0000  (  0.00%)\n",
            "\n",
            "================================================================================\n",
            "OVERALL PERFORMANCE METRICS\n",
            "================================================================================\n",
            "Overall Accuracy                          0.9947  ( 99.47%)\n",
            "F1 Score                                  0.9931\n",
            "Precision                                 0.0000  (  0.00%)\n",
            "Recall                                    0.0000  (  0.00%)\n",
            "\n",
            "================================================================================\n",
            "SUMMARY STATISTICS\n",
            "================================================================================\n",
            "Total Validation Samples                        0\n",
            "\n",
            "‚ùå ERROR: division by zero\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-189099889.py\", line 84, in display_saved_metrics\n",
            "    print(f\"{'Correctly Classified':40s} {tn + tp:8d}  ({((tn+tp)/total_samples)*100:6.2f}%)\")\n",
            "                                                          ~~~~~~~^^~~~~~~~~~~~~\n",
            "ZeroDivisionError: division by zero\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 7: Training Function (1 EPOCH, WITH COMPLETE METRICS)\n",
        "def train():\n",
        "    print(\"=\" * 60)\n",
        "    print(\"LOADING DATASET\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Load dataset - will fail if any line is corrupted\n",
        "    try:\n",
        "        df = load_clean_jsonl(\"final_dataset_no_emojis.jsonl\")\n",
        "    except ValueError as e:\n",
        "        print(f\"\\n‚ùå ERROR: {str(e)}\")\n",
        "        print(\"\\nüí° Fix your JSONL file and try again.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"VALIDATING DATASET\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    class_counts = df[\"label\"].value_counts().sort_index()\n",
        "    print(f\"Total samples: {len(df)}\")\n",
        "    print(f\"Class 0 (Human): {class_counts.get(0, 0)}\")\n",
        "    print(f\"Class 1 (AI): {class_counts.get(1, 0)}\")\n",
        "\n",
        "    # Validate both classes exist\n",
        "    if len(class_counts) < 2:\n",
        "        print(\"\\n‚ùå ERROR: Dataset must have BOTH classes!\")\n",
        "        print(f\"   Found only: {list(class_counts.index)}\")\n",
        "        print(f\"   Need: [0, 1]\")\n",
        "        return\n",
        "\n",
        "    if 0 not in class_counts or 1 not in class_counts:\n",
        "        print(\"\\n‚ùå ERROR: Missing class label!\")\n",
        "        print(f\"   Found labels: {list(class_counts.index)}\")\n",
        "        print(f\"   Need labels: [0, 1]\")\n",
        "        return\n",
        "\n",
        "    print(\"‚úÖ Dataset validation passed!\")\n",
        "\n",
        "    # Split data\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"SPLITTING DATA\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    train_df, val_df = train_test_split(\n",
        "        df, test_size=0.2, stratify=df[\"label\"], random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"Training samples: {len(train_df)}\")\n",
        "    print(f\"  - Class 0: {(train_df['label'] == 0).sum()}\")\n",
        "    print(f\"  - Class 1: {(train_df['label'] == 1).sum()}\")\n",
        "    print(f\"Validation samples: {len(val_df)}\")\n",
        "    print(f\"  - Class 0: {(val_df['label'] == 0).sum()}\")\n",
        "    print(f\"  - Class 1: {(val_df['label'] == 1).sum()}\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"LOADING TOKENIZER\")\n",
        "    print(\"=\" * 60)\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "    print(\"‚úÖ Tokenizer loaded\")\n",
        "\n",
        "    # Create datasets\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"CREATING DATASETS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    train_ds = HybridDataset(\n",
        "        train_df.text.tolist(),\n",
        "        train_df.label.tolist(),\n",
        "        tokenizer,\n",
        "        train_mode=True\n",
        "    )\n",
        "\n",
        "    val_ds = HybridDataset(\n",
        "        val_df.text.tolist(),\n",
        "        val_df.label.tolist(),\n",
        "        tokenizer,\n",
        "        train_mode=False,\n",
        "        feature_stats=train_ds.get_feature_stats()\n",
        "    )\n",
        "\n",
        "    print(\"‚úÖ Datasets created\")\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=32)\n",
        "\n",
        "    # Calculate class weights\n",
        "    counts = train_df[\"label\"].value_counts().sort_index()\n",
        "    total = len(train_df)\n",
        "\n",
        "    raw_weights = []\n",
        "    for i in [0, 1]:\n",
        "        if i in counts:\n",
        "            raw_weights.append(total / (2 * counts[i]))\n",
        "        else:\n",
        "            raw_weights.append(1.0)\n",
        "\n",
        "    # Dampen extreme weights\n",
        "    class_weights = torch.tensor(\n",
        "        [(w ** 0.5) for w in raw_weights],\n",
        "        dtype=torch.float\n",
        "    ).to(device)\n",
        "\n",
        "    print(f\"\\nClass weights (dampened): {class_weights.tolist()}\")\n",
        "\n",
        "    # Initialize model\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"INITIALIZING MODEL\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    model = RobertaWithFeatures(dropout_rate=0.6).to(device)\n",
        "    print(\"‚úÖ Model initialized\")\n",
        "\n",
        "    # Count trainable parameters\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"Frozen parameters: {total_params - trainable_params:,}\")\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-6, weight_decay=0.01)\n",
        "\n",
        "    # Loss function\n",
        "    loss_fn = nn.CrossEntropyLoss(\n",
        "        weight=class_weights,\n",
        "        label_smoothing=0.2\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"TRAINING (1 EPOCH)\")\n",
        "    print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    num_batches = len(train_loader)\n",
        "\n",
        "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(\n",
        "            batch[\"input_ids\"].to(device),\n",
        "            batch[\"attention_mask\"].to(device),\n",
        "            batch[\"features\"].to(device)\n",
        "        )\n",
        "\n",
        "        loss = loss_fn(logits, batch[\"labels\"].to(device))\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    print(f\"\\nTrain Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"VALIDATION\")\n",
        "    print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "    model.eval()\n",
        "    preds, labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
        "            out = model(\n",
        "                batch[\"input_ids\"].to(device),\n",
        "                batch[\"attention_mask\"].to(device),\n",
        "                batch[\"features\"].to(device)\n",
        "            )\n",
        "            preds.extend(out.argmax(1).cpu().numpy())\n",
        "            labels.extend(batch[\"labels\"].numpy())\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    labels_np = np.array(labels)\n",
        "    preds_np = np.array(preds)\n",
        "\n",
        "    # Calculate confusion matrix components\n",
        "    # Class 0 = Human, Class 1 = AI\n",
        "    true_negatives = int(np.sum((labels_np == 0) & (preds_np == 0)))\n",
        "    false_positives = int(np.sum((labels_np == 0) & (preds_np == 1)))\n",
        "    false_negatives = int(np.sum((labels_np == 1) & (preds_np == 0)))\n",
        "    true_positives = int(np.sum((labels_np == 1) & (preds_np == 1)))\n",
        "\n",
        "    # Calculate rates\n",
        "    total_human = np.sum(labels_np == 0)\n",
        "    total_ai = np.sum(labels_np == 1)\n",
        "\n",
        "    true_positive_rate = float(true_positives / total_ai if total_ai > 0 else 0)\n",
        "    false_positive_rate = float(false_positives / total_human if total_human > 0 else 0)\n",
        "    true_negative_rate = float(true_negatives / total_human if total_human > 0 else 0)\n",
        "    false_negative_rate = float(false_negatives / total_ai if total_ai > 0 else 0)\n",
        "\n",
        "    # Calculate standard metrics\n",
        "    f1 = float(f1_score(labels, preds, average='binary'))\n",
        "    acc = float(accuracy_score(labels, preds))\n",
        "\n",
        "    class0_mask = labels_np == 0\n",
        "    class1_mask = labels_np == 1\n",
        "\n",
        "    class0_acc = float(accuracy_score(labels_np[class0_mask], preds_np[class0_mask]))\n",
        "    class1_acc = float(accuracy_score(labels_np[class1_mask], preds_np[class1_mask]))\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    precision = float(true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0)\n",
        "    recall = true_positive_rate\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"CONFUSION MATRIX\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"                    Predicted\")\n",
        "    print(f\"                Human    AI\")\n",
        "    print(f\"Actual Human    {true_negatives:5d}  {false_positives:5d}\")\n",
        "    print(f\"Actual AI       {false_negatives:5d}  {true_positives:5d}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"DETAILED METRICS\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\\nüìä Count Metrics:\")\n",
        "    print(f\"   True Negatives (TN):  {true_negatives:5d} (Human correctly identified)\")\n",
        "    print(f\"   False Positives (FP): {false_positives:5d} (Human wrongly labeled as AI)\")\n",
        "    print(f\"   False Negatives (FN): {false_negatives:5d} (AI wrongly labeled as Human)\")\n",
        "    print(f\"   True Positives (TP):  {true_positives:5d} (AI correctly identified)\")\n",
        "\n",
        "    print(f\"\\nüìà Rate Metrics:\")\n",
        "    print(f\"   True Positive Rate (TPR/Recall/Sensitivity): {true_positive_rate:.4f} ({true_positive_rate*100:.2f}%)\")\n",
        "    print(f\"   False Positive Rate (FPR):                   {false_positive_rate:.4f} ({false_positive_rate*100:.2f}%)\")\n",
        "    print(f\"   True Negative Rate (TNR/Specificity):        {true_negative_rate:.4f} ({true_negative_rate*100:.2f}%)\")\n",
        "    print(f\"   False Negative Rate (FNR):                   {false_negative_rate:.4f} ({false_negative_rate*100:.2f}%)\")\n",
        "\n",
        "    print(f\"\\nüéØ Overall Metrics:\")\n",
        "    print(f\"   Overall Accuracy:        {acc:.4f} ({acc*100:.2f}%)\")\n",
        "    print(f\"   F1 Score:                {f1:.4f}\")\n",
        "    print(f\"   Precision:               {precision:.4f} ({precision*100:.2f}%)\")\n",
        "    print(f\"   Recall:                  {recall:.4f} ({recall*100:.2f}%)\")\n",
        "\n",
        "    print(f\"\\nüìã Per-Class Accuracy:\")\n",
        "    print(f\"   Class 0 (Human): {class0_acc:.4f} ({class0_acc*100:.2f}%)\")\n",
        "    print(f\"   Class 1 (AI):    {class1_acc:.4f} ({class1_acc*100:.2f}%)\")\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Save model with ALL metrics properly\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"SAVING MODEL\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    torch.save({\n",
        "        'model_state': model.state_dict(),\n",
        "        'feature_stats': train_ds.get_feature_stats(),\n",
        "        'accuracy': acc,\n",
        "        'f1_score': f1,\n",
        "        'confusion_matrix': {\n",
        "            'true_negatives': true_negatives,\n",
        "            'false_positives': false_positives,\n",
        "            'false_negatives': false_negatives,\n",
        "            'true_positives': true_positives\n",
        "        },\n",
        "        'rates': {\n",
        "            'true_positive_rate': true_positive_rate,\n",
        "            'false_positive_rate': false_positive_rate,\n",
        "            'true_negative_rate': true_negative_rate,\n",
        "            'false_negative_rate': false_negative_rate\n",
        "        },\n",
        "        'metrics': {\n",
        "            'precision': precision,\n",
        "            'recall': recall\n",
        "        }\n",
        "    }, \"/content/drive/MyDrive/TraceAI_Hybrid_Best.pt\")\n",
        "\n",
        "    tokenizer.save_pretrained(\"/content/drive/MyDrive/TraceAI_Hybrid_Best\")\n",
        "\n",
        "    print(f\"‚úÖ Model saved to: /content/drive/MyDrive/TraceAI_Hybrid_Best.pt\")\n",
        "    print(f\"‚úÖ Tokenizer saved to: /content/drive/MyDrive/TraceAI_Hybrid_Best/\")\n",
        "    print(f\"‚úÖ All metrics saved in checkpoint\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"üí° TIP: Run display_saved_metrics() to view these results anytime!\")\n",
        "    print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "iMFcDA3EXeNl"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWmEbsX_XjeK",
        "outputId": "0733551b-112c-4dbc-ccdc-31ca76b7b238"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "LOADING DATASET\n",
            "============================================================\n",
            "‚úÖ Successfully loaded 20788 samples\n",
            "Class distribution: {0: 12788, 1: 8000}\n",
            "\n",
            "============================================================\n",
            "VALIDATING DATASET\n",
            "============================================================\n",
            "Total samples: 20788\n",
            "Class 0 (Human): 12788\n",
            "Class 1 (AI): 8000\n",
            "‚úÖ Dataset validation passed!\n",
            "\n",
            "============================================================\n",
            "SPLITTING DATA\n",
            "============================================================\n",
            "Training samples: 16630\n",
            "  - Class 0: 10230\n",
            "  - Class 1: 6400\n",
            "Validation samples: 4158\n",
            "  - Class 0: 2558\n",
            "  - Class 1: 1600\n",
            "\n",
            "============================================================\n",
            "LOADING TOKENIZER\n",
            "============================================================\n",
            "‚úÖ Tokenizer loaded\n",
            "\n",
            "============================================================\n",
            "CREATING DATASETS\n",
            "============================================================\n",
            "‚úÖ Datasets created\n",
            "\n",
            "Class weights (dampened): [0.9015572667121887, 1.1398327350616455]\n",
            "\n",
            "============================================================\n",
            "INITIALIZING MODEL\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model initialized\n",
            "Trainable parameters: 29,172,226\n",
            "Frozen parameters: 95,703,552\n",
            "\n",
            "============================================================\n",
            "TRAINING (1 EPOCH)\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1040/1040 [06:37<00:00,  2.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.5246\n",
            "\n",
            "============================================================\n",
            "VALIDATION\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 130/130 [00:58<00:00,  2.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "CONFUSION MATRIX\n",
            "============================================================\n",
            "                    Predicted\n",
            "                Human    AI\n",
            "Actual Human     2534     24\n",
            "Actual AI           8   1592\n",
            "\n",
            "============================================================\n",
            "DETAILED METRICS\n",
            "============================================================\n",
            "\n",
            "üìä Count Metrics:\n",
            "   True Negatives (TN):   2534 (Human correctly identified)\n",
            "   False Positives (FP):    24 (Human wrongly labeled as AI)\n",
            "   False Negatives (FN):     8 (AI wrongly labeled as Human)\n",
            "   True Positives (TP):   1592 (AI correctly identified)\n",
            "\n",
            "üìà Rate Metrics:\n",
            "   True Positive Rate (TPR/Recall/Sensitivity): 0.9950 (99.50%)\n",
            "   False Positive Rate (FPR):                   0.0094 (0.94%)\n",
            "   True Negative Rate (TNR/Specificity):        0.9906 (99.06%)\n",
            "   False Negative Rate (FNR):                   0.0050 (0.50%)\n",
            "\n",
            "üéØ Overall Metrics:\n",
            "   Overall Accuracy:        0.9923 (99.23%)\n",
            "   F1 Score:                0.9900\n",
            "   Precision:               0.9851 (98.51%)\n",
            "   Recall:                  0.9950 (99.50%)\n",
            "\n",
            "üìã Per-Class Accuracy:\n",
            "   Class 0 (Human): 0.9906 (99.06%)\n",
            "   Class 1 (AI):    0.9950 (99.50%)\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "SAVING MODEL\n",
            "============================================================\n",
            "‚úÖ Model saved to: /content/drive/MyDrive/TraceAI_Hybrid_Best.pt\n",
            "‚úÖ Tokenizer saved to: /content/drive/MyDrive/TraceAI_Hybrid_Best/\n",
            "‚úÖ All metrics saved in checkpoint\n",
            "\n",
            "============================================================\n",
            "üí° TIP: Run display_saved_metrics() to view these results anytime!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_saved_metrics()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0XWTBBjZeko",
        "outputId": "6e726bef-7869-4b9c-8ff9-786503e880a4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "LOADING SAVED MODEL METRICS\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "CONFUSION MATRIX\n",
            "================================================================================\n",
            "                     Predicted\n",
            "                          Human         AI\n",
            "Actual Human               2534         24\n",
            "Actual AI                     8       1592\n",
            "\n",
            "================================================================================\n",
            "COUNT METRICS\n",
            "================================================================================\n",
            "True Negatives (TN)                          2534  (Human correctly identified)\n",
            "False Positives (FP)                           24  (Human wrongly labeled as AI)\n",
            "False Negatives (FN)                            8  (AI wrongly labeled as Human)\n",
            "True Positives (TP)                          1592  (AI correctly identified)\n",
            "\n",
            "================================================================================\n",
            "RATE METRICS (Percentages)\n",
            "================================================================================\n",
            "True Positive Rate (Sensitivity/Recall)   0.9950  ( 99.50%)\n",
            "False Positive Rate                       0.0094  (  0.94%)\n",
            "True Negative Rate (Specificity)          0.9906  ( 99.06%)\n",
            "False Negative Rate                       0.0050  (  0.50%)\n",
            "\n",
            "================================================================================\n",
            "OVERALL PERFORMANCE METRICS\n",
            "================================================================================\n",
            "Overall Accuracy                          0.9923  ( 99.23%)\n",
            "F1 Score                                  0.9900\n",
            "Precision                                 0.9851  ( 98.51%)\n",
            "Recall                                    0.9950  ( 99.50%)\n",
            "\n",
            "================================================================================\n",
            "SUMMARY STATISTICS\n",
            "================================================================================\n",
            "Total Validation Samples                     4158\n",
            "Correctly Classified                         4126  ( 99.23%)\n",
            "Misclassified                                  32  (  0.77%)\n",
            "\n",
            "================================================================================\n",
            "INTERPRETATION GUIDE\n",
            "================================================================================\n",
            "\n",
            "üìä Key Metrics Explained:\n",
            "\n",
            "TRUE POSITIVE RATE (Sensitivity/Recall):\n",
            "   ‚Üí How well the model identifies AI-generated text\n",
            "   ‚Üí Higher is better (ideally > 0.85)\n",
            "\n",
            "FALSE POSITIVE RATE:\n",
            "   ‚Üí How often human text is misclassified as AI\n",
            "   ‚Üí Lower is better (ideally < 0.15)\n",
            "\n",
            "TRUE NEGATIVE RATE (Specificity):\n",
            "   ‚Üí How well the model identifies human-written text\n",
            "   ‚Üí Higher is better (ideally > 0.85)\n",
            "\n",
            "FALSE NEGATIVE RATE:\n",
            "   ‚Üí How often AI text is misclassified as human\n",
            "   ‚Üí Lower is better (ideally < 0.15)\n",
            "\n",
            "PRECISION:\n",
            "   ‚Üí When model predicts AI, how often is it correct?\n",
            "   ‚Üí Higher is better\n",
            "\n",
            "RECALL (same as TPR):\n",
            "   ‚Üí Of all actual AI texts, how many did we catch?\n",
            "   ‚Üí Higher is better\n",
            "\n",
            "F1 SCORE:\n",
            "   ‚Üí Balanced measure of precision and recall\n",
            "   ‚Üí Higher is better (ideally > 0.80)\n",
            "        \n",
            "================================================================================\n",
            "‚úÖ METRICS LOADED SUCCESSFULLY\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}